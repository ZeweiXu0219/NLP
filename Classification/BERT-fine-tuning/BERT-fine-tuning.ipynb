{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip3 install transformers torch"
      ],
      "metadata": {
        "id": "gzWq5nQyEvIi",
        "outputId": "b47cda9d-2913-45b6-b182-65cf1056db8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "CD2myIqaE5d_",
        "outputId": "d96f9b97-949b-4d92-b82e-1c3abb783144",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Library"
      ],
      "metadata": {
        "id": "ZJ0gIcwaFNWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "1VQ5n0m4E3l5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IMDB review\n",
        "train_data = pd.read_csv(\"/content/drive/MyDrive/sentiment_analysis_dataset/IMDB/Train.csv\")\n",
        "test_data = pd.read_csv(\"/content/drive/MyDrive/sentiment_analysis_dataset/IMDB/Test.csv\")\n",
        "valid_data = pd.read_csv(\"/content/drive/MyDrive/sentiment_analysis_dataset/IMDB/Valid.csv\")"
      ],
      "metadata": {
        "id": "qes2oGs2HYGq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "id": "AHqCBSmkh1EQ",
        "outputId": "781899b9-905d-46b8-9781-12de7a4d98df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  label\n",
              "0  I grew up (b. 1965) watching and loving the Th...      0\n",
              "1  When I put this movie in my DVD player, and sa...      0\n",
              "2  Why do people who do not know what a particula...      0\n",
              "3  Even though I have great interest in Biblical ...      0\n",
              "4  Im a die hard Dads Army fan and nothing will e...      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-84f46541-5fbf-414d-8a3d-6d4868a93d1a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I grew up (b. 1965) watching and loving the Th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>When I put this movie in my DVD player, and sa...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Why do people who do not know what a particula...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Even though I have great interest in Biblical ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Im a die hard Dads Army fan and nothing will e...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-84f46541-5fbf-414d-8a3d-6d4868a93d1a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-84f46541-5fbf-414d-8a3d-6d4868a93d1a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-84f46541-5fbf-414d-8a3d-6d4868a93d1a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b52a713a-e1ca-469d-8b9a-13aed8c05149\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b52a713a-e1ca-469d-8b9a-13aed8c05149')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b52a713a-e1ca-469d-8b9a-13aed8c05149 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_data",
              "summary": "{\n  \"name\": \"train_data\",\n  \"rows\": 40000,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 39723,\n        \"samples\": [\n          \"There are similarities between Ray Lawrence's \\\"Jindabyne\\\" and his last movie \\\"Lantana\\\" \\u0096 a dead body and its repercussions for already dysfunctional lives. But whereas \\\"Lantana\\\" offered some hope and resolution, \\\"Jindabyne\\\" leaves everything unresolved in a bleak way that will leave most viewers unsatisfied, perhaps even cheated.<br /><br />The storyline - the aftermath of a fisherman's discovery of a corpse floating in a remote river - is based on a short story by Raymond Carver. It became an element in Robert Altman's classic 1993 ensemble \\\"Short Cuts\\\". Lawrence uses this theme for an exploration and exposition of relationships within a small Australian community under stress. The movie poses some moral questions \\\"Would you let the discovery of a dead body ruin your good weekend?\\\" and more poignantly for Australians \\\"Would it make any difference if the dead person was an aboriginal?\\\" The acting, especially by Gabriel Byrne and Laura Linney, is commendable. And there are elements of mysticism reinforced by haunting music, not unlike \\\"Picnic at Hanging Rock\\\".<br /><br />If all this sounds like the basis for a great movie - be prepared for a let down, the pace is very slow and the murder is shown near the beginning, thereby eliminating the element of mystery. And so we are left with these desolate lives and a blank finale.\",\n          \"Hammer House of Horror: Witching Time is set in rural England on Woodstock farm where stressed musician David Winter (Jon Finch) lives with his actress wife Mary (Prunella Gee) & is currently composing the music for a horror film. One night while looking for his dog Billy David finds a mysterious woman in his barn, calling herself Lucinda Jessop (Patricia Quinn) she claims to be a witch who has transported herself from 300 years in the past to now. Obviously rather sceptical David has a hard time believing her so he locks her in a room in his farmhouse & calls his doctor Charles (Ian McCulloch) to come examine her, however once he arrives & they enter the room Lucinda has disappeared. Charles puts it down to David drinking too much but over the next few day strange & disturbing things begin to happen to David & Mary...<br /><br />Witching Time was episode 1 from the short lived British anthology horror series produced by Hammer studios for TV & originally aired here in the UK during September 1980, the first of two Hammer House of Horror episodes to be directed by Don Leaver (episode 13 The Mark of Satan being the other) I actually rather liked this. As a series Hammer House of Horror dealt with various different themes & were all unconnected to each other except in name & unsurprisingly Watching Time is a sinister & effective little tale about a witch, the script by Anthony Read benefits from it's slight 50 odd minute duration & moves along at a nice pace. The character's are pretty good as is the dialogue, there are some nice scenes here & I liked the way it never quite reveals whether David & Mary are going crazy or not. I think it's a well structured, entertaining & reasonably creepy horror themed TV show that I enjoyed more than I thought I would.<br /><br />Being made for British TV meant the boys at Hammer had a lower budget than usual, if that was even possible, & as such there is no gorgeous period settings here as in their most well know Frankenstein & Dracula films although the contemporary English setting does give it a certain atmosphere that you can relate to a bit more. Another TV based restriction is that the exploitation levels are lower than you might hope for, there's some nudity & gore but not much although I didn't mind too much as the story here is pretty good. It's well made for what it is & Hammer's experience on their feature films probably helped make these look pretty good, the acting is good as well with genre favourite Ian McCulloch making a bit-part appearance.<br /><br />Witching Time is a good start to the Hammer House of Horror series, as a 50 minute piece of British TV it's pretty damned good, now why don't they make show's like this over here anymore?\",\n          \"What a great cast for this movie. The timing was excellent and there were so many clever lines-several times I was still laughing minutes after they were delivered. I found Manna From Heaven to have some surprising moments and while there were things I was thinking would happen, the way they came together was anything but predictable. This movie is about hope and righting wrongs. I left the theater feeling inspired to do the right thing. Bravo to the Five Sisters.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['label'].value_counts()\n",
        "# balanced data"
      ],
      "metadata": {
        "id": "VV6YXqaphKKC",
        "outputId": "adef901a-09a0-4150-fa40-4d22dedc86a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label\n",
              "0    20019\n",
              "1    19981\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>19981</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max_len(text):\n",
        "  \"\"\"\n",
        "  get max length of text\n",
        "  \"\"\"\n",
        "  max_len = 0\n",
        "  for sentence in text:\n",
        "    max_len = max(max_len,len(sentence))\n",
        "  return max_len\n",
        "\n",
        "print(get_max_len(train_data['text'].tolist()))"
      ],
      "metadata": {
        "id": "4r4ghvSNKKHF",
        "outputId": "4af85aa8-3535-412a-89ad-2b65bb79551a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13704\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Dataset\n",
        "\n",
        "- Dataset is used to generate batch data during training"
      ],
      "metadata": {
        "id": "GlpcqOQyFf9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=True,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "4eEkesxiFfnB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokenizer_and_model(model_name):\n",
        "  tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "  model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "  return tokenizer, model"
      ],
      "metadata": {
        "id": "fJnx-P6LM7Sa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, valid_dataloader, device):\n",
        "  model.eval()\n",
        "  predictions, true_labels = [],[]\n",
        "  with torch.no_grad():\n",
        "    for batch in valid_dataloader:\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      labels = batch['label'].to(device)\n",
        "\n",
        "      pred_output = model(input_ids, attention_mask)\n",
        "      logits = pred_output.logits\n",
        "      logits = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "      labels = labels.cpu().numpy()\n",
        "      predictions.extend(logits)\n",
        "      true_labels.extend(labels)\n",
        "  return accuracy_score(true_labels, predictions)"
      ],
      "metadata": {
        "id": "ddByGJEHlE35"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_dataloader, valid_dataloader, epochs, inital_lr, device, step=20):\n",
        "  optimizer = AdamW(model.parameters(), lr=inital_lr, correct_bias=False)\n",
        "  model = model.to(device)\n",
        "  trained_data = 0\n",
        "  step_ = 0\n",
        "  for epoch in range(epochs):\n",
        "    # forward\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_loss_list = []\n",
        "    for batch in train_dataloader:\n",
        "      optimizer.zero_grad()\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      labels = batch['label'].to(device)\n",
        "\n",
        "      model_output = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "      loss = model_output.loss\n",
        "      total_loss += loss.item()\n",
        "      total_loss_list.append(loss.item())\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      trained_data += batch_size\n",
        "      step_ += 1\n",
        "      if step_ % step == 0:\n",
        "        print(f'Epoch: {epoch + 1}/{epochs}, trained_data:  {trained_data} / {epochs*batch_size*len(train_dataloader)}, Loss(avg): {total_loss / step*batch_size}')\n",
        "        total_loss = 0\n",
        "    val_accuracy = evaluate_model(model, valid_dataloader, device)\n",
        "    print(f'Validation Accuracy: {val_accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "JALYXMhdjSMM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train_data['text'].tolist()\n",
        "y_train = train_data['label'].tolist()\n",
        "X_valid = valid_data['text'].tolist()\n",
        "y_valid = valid_data['label'].tolist()\n",
        "X_test = test_data['text'].tolist()\n",
        "y_test = test_data['label'].tolist()"
      ],
      "metadata": {
        "id": "FIFFyKHemZg7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'bert-base-uncased'\n",
        "tokenizer, model = get_tokenizer_and_model(model_name)\n",
        "\n",
        "# max_train_len = get_max_len(X_train)\n",
        "# max_valid_len = get_max_len(X_valid)\n",
        "# max_test_len = get_max_len(X_test)\n",
        "\n",
        "# bert max_len is 512\n",
        "max_len = 128\n",
        "epoch = 3\n",
        "lr = 2e-5\n",
        "batch_size = 16\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_dataset = MyDataset(X_train, y_train, tokenizer, max_len)\n",
        "val_dataset = MyDataset(X_valid, y_valid, tokenizer, max_len)\n",
        "\n",
        "# 定义数据加载器\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "train_model(model, train_loader, val_loader, epochs=epoch, inital_lr = lr, device = device)"
      ],
      "metadata": {
        "id": "z_cBlyYnmt-K",
        "outputId": "a659446f-2d4e-487b-90ed-c8a41f0bd9da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/3, trained_data:  320 / 120000, Loss(avg): 11.398277139663696\n",
            "Epoch: 1/3, trained_data:  640 / 120000, Loss(avg): 11.334536075592041\n",
            "Epoch: 1/3, trained_data:  960 / 120000, Loss(avg): 10.904706335067749\n",
            "Epoch: 1/3, trained_data:  1280 / 120000, Loss(avg): 9.309746384620667\n",
            "Epoch: 1/3, trained_data:  1600 / 120000, Loss(avg): 7.7105267405509945\n",
            "Epoch: 1/3, trained_data:  1920 / 120000, Loss(avg): 8.736096453666686\n",
            "Epoch: 1/3, trained_data:  2240 / 120000, Loss(avg): 6.8486563205719\n",
            "Epoch: 1/3, trained_data:  2560 / 120000, Loss(avg): 7.605550789833069\n",
            "Epoch: 1/3, trained_data:  2880 / 120000, Loss(avg): 6.564855015277862\n",
            "Epoch: 1/3, trained_data:  3200 / 120000, Loss(avg): 7.296554946899414\n",
            "Epoch: 1/3, trained_data:  3520 / 120000, Loss(avg): 6.934622478485108\n",
            "Epoch: 1/3, trained_data:  3840 / 120000, Loss(avg): 6.3076066970825195\n",
            "Epoch: 1/3, trained_data:  4160 / 120000, Loss(avg): 6.477697348594665\n",
            "Epoch: 1/3, trained_data:  4480 / 120000, Loss(avg): 5.743533277511597\n",
            "Epoch: 1/3, trained_data:  4800 / 120000, Loss(avg): 5.650297451019287\n",
            "Epoch: 1/3, trained_data:  5120 / 120000, Loss(avg): 6.439473831653595\n",
            "Epoch: 1/3, trained_data:  5440 / 120000, Loss(avg): 5.7669856429100035\n",
            "Epoch: 1/3, trained_data:  5760 / 120000, Loss(avg): 6.462544953823089\n",
            "Epoch: 1/3, trained_data:  6080 / 120000, Loss(avg): 5.89555790424347\n",
            "Epoch: 1/3, trained_data:  6400 / 120000, Loss(avg): 5.894215619564056\n",
            "Epoch: 1/3, trained_data:  6720 / 120000, Loss(avg): 5.612385892868042\n",
            "Epoch: 1/3, trained_data:  7040 / 120000, Loss(avg): 4.984290778636932\n",
            "Epoch: 1/3, trained_data:  7360 / 120000, Loss(avg): 4.832853943109512\n",
            "Epoch: 1/3, trained_data:  7680 / 120000, Loss(avg): 5.344917154312133\n",
            "Epoch: 1/3, trained_data:  8000 / 120000, Loss(avg): 5.349114620685578\n",
            "Epoch: 1/3, trained_data:  8320 / 120000, Loss(avg): 4.051580554246902\n",
            "Epoch: 1/3, trained_data:  8640 / 120000, Loss(avg): 5.664882373809815\n",
            "Epoch: 1/3, trained_data:  8960 / 120000, Loss(avg): 6.186152720451355\n",
            "Epoch: 1/3, trained_data:  9280 / 120000, Loss(avg): 5.72443521618843\n",
            "Epoch: 1/3, trained_data:  9600 / 120000, Loss(avg): 5.3686067938804625\n",
            "Epoch: 1/3, trained_data:  9920 / 120000, Loss(avg): 6.482439112663269\n",
            "Epoch: 1/3, trained_data:  10240 / 120000, Loss(avg): 4.894343221187592\n",
            "Epoch: 1/3, trained_data:  10560 / 120000, Loss(avg): 4.738930988311767\n",
            "Epoch: 1/3, trained_data:  10880 / 120000, Loss(avg): 4.467904627323151\n",
            "Epoch: 1/3, trained_data:  11200 / 120000, Loss(avg): 5.202876377105713\n",
            "Epoch: 1/3, trained_data:  11520 / 120000, Loss(avg): 4.6703928172588345\n",
            "Epoch: 1/3, trained_data:  11840 / 120000, Loss(avg): 5.090919494628906\n",
            "Epoch: 1/3, trained_data:  12160 / 120000, Loss(avg): 4.512488943338394\n",
            "Epoch: 1/3, trained_data:  12480 / 120000, Loss(avg): 5.6847178637981415\n",
            "Epoch: 1/3, trained_data:  12800 / 120000, Loss(avg): 4.774961483478546\n",
            "Epoch: 1/3, trained_data:  13120 / 120000, Loss(avg): 5.192055869102478\n",
            "Epoch: 1/3, trained_data:  13440 / 120000, Loss(avg): 4.780153477191925\n",
            "Epoch: 1/3, trained_data:  13760 / 120000, Loss(avg): 4.679960519075394\n",
            "Epoch: 1/3, trained_data:  14080 / 120000, Loss(avg): 6.41193677186966\n",
            "Epoch: 1/3, trained_data:  14400 / 120000, Loss(avg): 5.784034585952758\n",
            "Epoch: 1/3, trained_data:  14720 / 120000, Loss(avg): 4.526315271854401\n",
            "Epoch: 1/3, trained_data:  15040 / 120000, Loss(avg): 5.103270804882049\n",
            "Epoch: 1/3, trained_data:  15360 / 120000, Loss(avg): 4.882923150062561\n",
            "Epoch: 1/3, trained_data:  15680 / 120000, Loss(avg): 4.719487541913987\n",
            "Epoch: 1/3, trained_data:  16000 / 120000, Loss(avg): 4.8852979183197025\n",
            "Epoch: 1/3, trained_data:  16320 / 120000, Loss(avg): 4.808189326524735\n",
            "Epoch: 1/3, trained_data:  16640 / 120000, Loss(avg): 4.5442567586898805\n",
            "Epoch: 1/3, trained_data:  16960 / 120000, Loss(avg): 4.531044882535935\n",
            "Epoch: 1/3, trained_data:  17280 / 120000, Loss(avg): 5.620974391698837\n",
            "Epoch: 1/3, trained_data:  17600 / 120000, Loss(avg): 4.865413010120392\n",
            "Epoch: 1/3, trained_data:  17920 / 120000, Loss(avg): 5.333464109897614\n",
            "Epoch: 1/3, trained_data:  18240 / 120000, Loss(avg): 4.900258088111878\n",
            "Epoch: 1/3, trained_data:  18560 / 120000, Loss(avg): 4.507062888145446\n",
            "Epoch: 1/3, trained_data:  18880 / 120000, Loss(avg): 5.33860536813736\n",
            "Epoch: 1/3, trained_data:  19200 / 120000, Loss(avg): 4.70825697183609\n",
            "Epoch: 1/3, trained_data:  19520 / 120000, Loss(avg): 5.193924963474274\n",
            "Epoch: 1/3, trained_data:  19840 / 120000, Loss(avg): 4.225269150733948\n",
            "Epoch: 1/3, trained_data:  20160 / 120000, Loss(avg): 3.6910596996545793\n",
            "Epoch: 1/3, trained_data:  20480 / 120000, Loss(avg): 4.38316330909729\n",
            "Epoch: 1/3, trained_data:  20800 / 120000, Loss(avg): 5.387501060962677\n",
            "Epoch: 1/3, trained_data:  21120 / 120000, Loss(avg): 4.518387937545777\n",
            "Epoch: 1/3, trained_data:  21440 / 120000, Loss(avg): 4.844968241453171\n",
            "Epoch: 1/3, trained_data:  21760 / 120000, Loss(avg): 3.7897822976112367\n",
            "Epoch: 1/3, trained_data:  22080 / 120000, Loss(avg): 4.681726545095444\n",
            "Epoch: 1/3, trained_data:  22400 / 120000, Loss(avg): 5.308527675271034\n",
            "Epoch: 1/3, trained_data:  22720 / 120000, Loss(avg): 4.043589305877686\n",
            "Epoch: 1/3, trained_data:  23040 / 120000, Loss(avg): 4.708152788877487\n",
            "Epoch: 1/3, trained_data:  23360 / 120000, Loss(avg): 4.969868692755699\n",
            "Epoch: 1/3, trained_data:  23680 / 120000, Loss(avg): 4.796877235174179\n",
            "Epoch: 1/3, trained_data:  24000 / 120000, Loss(avg): 4.029418641328812\n",
            "Epoch: 1/3, trained_data:  24320 / 120000, Loss(avg): 3.641562694311142\n",
            "Epoch: 1/3, trained_data:  24640 / 120000, Loss(avg): 4.80392894744873\n",
            "Epoch: 1/3, trained_data:  24960 / 120000, Loss(avg): 3.972034972906113\n",
            "Epoch: 1/3, trained_data:  25280 / 120000, Loss(avg): 4.011479371786118\n",
            "Epoch: 1/3, trained_data:  25600 / 120000, Loss(avg): 5.0166405737400055\n",
            "Epoch: 1/3, trained_data:  25920 / 120000, Loss(avg): 4.178750687837601\n",
            "Epoch: 1/3, trained_data:  26240 / 120000, Loss(avg): 4.599801364541054\n",
            "Epoch: 1/3, trained_data:  26560 / 120000, Loss(avg): 5.303162103891372\n",
            "Epoch: 1/3, trained_data:  26880 / 120000, Loss(avg): 4.633572417497635\n",
            "Epoch: 1/3, trained_data:  27200 / 120000, Loss(avg): 5.242699018120765\n",
            "Epoch: 1/3, trained_data:  27520 / 120000, Loss(avg): 4.248046320676804\n",
            "Epoch: 1/3, trained_data:  27840 / 120000, Loss(avg): 4.301829677820206\n",
            "Epoch: 1/3, trained_data:  28160 / 120000, Loss(avg): 4.468682271242142\n",
            "Epoch: 1/3, trained_data:  28480 / 120000, Loss(avg): 4.301389366388321\n",
            "Epoch: 1/3, trained_data:  28800 / 120000, Loss(avg): 4.460400402545929\n",
            "Epoch: 1/3, trained_data:  29120 / 120000, Loss(avg): 4.591907846927643\n",
            "Epoch: 1/3, trained_data:  29440 / 120000, Loss(avg): 4.212009769678116\n",
            "Epoch: 1/3, trained_data:  29760 / 120000, Loss(avg): 3.5911950886249544\n",
            "Epoch: 1/3, trained_data:  30080 / 120000, Loss(avg): 5.760716593265533\n",
            "Epoch: 1/3, trained_data:  30400 / 120000, Loss(avg): 4.781368148326874\n",
            "Epoch: 1/3, trained_data:  30720 / 120000, Loss(avg): 4.057862037420273\n",
            "Epoch: 1/3, trained_data:  31040 / 120000, Loss(avg): 4.527901399135589\n",
            "Epoch: 1/3, trained_data:  31360 / 120000, Loss(avg): 5.210737174749374\n",
            "Epoch: 1/3, trained_data:  31680 / 120000, Loss(avg): 4.810771238803864\n",
            "Epoch: 1/3, trained_data:  32000 / 120000, Loss(avg): 3.867719578742981\n",
            "Epoch: 1/3, trained_data:  32320 / 120000, Loss(avg): 3.971242916584015\n",
            "Epoch: 1/3, trained_data:  32640 / 120000, Loss(avg): 4.743972179293633\n",
            "Epoch: 1/3, trained_data:  32960 / 120000, Loss(avg): 4.997864603996277\n",
            "Epoch: 1/3, trained_data:  33280 / 120000, Loss(avg): 4.758068335056305\n",
            "Epoch: 1/3, trained_data:  33600 / 120000, Loss(avg): 4.569485086202621\n",
            "Epoch: 1/3, trained_data:  33920 / 120000, Loss(avg): 4.012097370624542\n",
            "Epoch: 1/3, trained_data:  34240 / 120000, Loss(avg): 5.099623513221741\n",
            "Epoch: 1/3, trained_data:  34560 / 120000, Loss(avg): 4.670310914516449\n",
            "Epoch: 1/3, trained_data:  34880 / 120000, Loss(avg): 4.307368075847625\n",
            "Epoch: 1/3, trained_data:  35200 / 120000, Loss(avg): 4.970750510692596\n",
            "Epoch: 1/3, trained_data:  35520 / 120000, Loss(avg): 4.265173548460007\n",
            "Epoch: 1/3, trained_data:  35840 / 120000, Loss(avg): 3.5537521481513976\n",
            "Epoch: 1/3, trained_data:  36160 / 120000, Loss(avg): 5.250259494781494\n",
            "Epoch: 1/3, trained_data:  36480 / 120000, Loss(avg): 3.4638249665498733\n",
            "Epoch: 1/3, trained_data:  36800 / 120000, Loss(avg): 4.386917221546173\n",
            "Epoch: 1/3, trained_data:  37120 / 120000, Loss(avg): 4.816856229305268\n",
            "Epoch: 1/3, trained_data:  37440 / 120000, Loss(avg): 4.368916320800781\n",
            "Epoch: 1/3, trained_data:  37760 / 120000, Loss(avg): 5.0348814725875854\n",
            "Epoch: 1/3, trained_data:  38080 / 120000, Loss(avg): 4.778925347328186\n",
            "Epoch: 1/3, trained_data:  38400 / 120000, Loss(avg): 3.6619073688983916\n",
            "Epoch: 1/3, trained_data:  38720 / 120000, Loss(avg): 3.900577101111412\n",
            "Epoch: 1/3, trained_data:  39040 / 120000, Loss(avg): 5.101821213960648\n",
            "Epoch: 1/3, trained_data:  39360 / 120000, Loss(avg): 4.607288646697998\n",
            "Epoch: 1/3, trained_data:  39680 / 120000, Loss(avg): 4.923084139823914\n",
            "Epoch: 1/3, trained_data:  40000 / 120000, Loss(avg): 4.882650423049927\n",
            "Validation Accuracy: 0.8850\n",
            "Epoch: 2/3, trained_data:  40320 / 120000, Loss(avg): 3.388301706314087\n",
            "Epoch: 2/3, trained_data:  40640 / 120000, Loss(avg): 2.1221864692866803\n",
            "Epoch: 2/3, trained_data:  40960 / 120000, Loss(avg): 3.588118926435709\n",
            "Epoch: 2/3, trained_data:  41280 / 120000, Loss(avg): 3.0739948868751528\n",
            "Epoch: 2/3, trained_data:  41600 / 120000, Loss(avg): 2.684809112548828\n",
            "Epoch: 2/3, trained_data:  41920 / 120000, Loss(avg): 2.3789480566978454\n",
            "Epoch: 2/3, trained_data:  42240 / 120000, Loss(avg): 2.971105207502842\n",
            "Epoch: 2/3, trained_data:  42560 / 120000, Loss(avg): 3.474411791563034\n",
            "Epoch: 2/3, trained_data:  42880 / 120000, Loss(avg): 3.03045873939991\n",
            "Epoch: 2/3, trained_data:  43200 / 120000, Loss(avg): 2.33884457051754\n",
            "Epoch: 2/3, trained_data:  43520 / 120000, Loss(avg): 2.6783032428473232\n",
            "Epoch: 2/3, trained_data:  43840 / 120000, Loss(avg): 2.442272301018238\n",
            "Epoch: 2/3, trained_data:  44160 / 120000, Loss(avg): 3.1408129215240477\n",
            "Epoch: 2/3, trained_data:  44480 / 120000, Loss(avg): 3.0806364193558693\n",
            "Epoch: 2/3, trained_data:  44800 / 120000, Loss(avg): 2.2984011128544806\n",
            "Epoch: 2/3, trained_data:  45120 / 120000, Loss(avg): 3.1437559902667997\n",
            "Epoch: 2/3, trained_data:  45440 / 120000, Loss(avg): 2.7102702513337134\n",
            "Epoch: 2/3, trained_data:  45760 / 120000, Loss(avg): 3.1130755245685577\n",
            "Epoch: 2/3, trained_data:  46080 / 120000, Loss(avg): 2.832403725385666\n",
            "Epoch: 2/3, trained_data:  46400 / 120000, Loss(avg): 2.1590237714350224\n",
            "Epoch: 2/3, trained_data:  46720 / 120000, Loss(avg): 2.5510169088840486\n",
            "Epoch: 2/3, trained_data:  47040 / 120000, Loss(avg): 2.6216648012399673\n",
            "Epoch: 2/3, trained_data:  47360 / 120000, Loss(avg): 2.933062931895256\n",
            "Epoch: 2/3, trained_data:  47680 / 120000, Loss(avg): 2.178074786067009\n",
            "Epoch: 2/3, trained_data:  48000 / 120000, Loss(avg): 2.1232047885656358\n",
            "Epoch: 2/3, trained_data:  48320 / 120000, Loss(avg): 2.5044289141893388\n",
            "Epoch: 2/3, trained_data:  48640 / 120000, Loss(avg): 2.2540546957403422\n",
            "Epoch: 2/3, trained_data:  48960 / 120000, Loss(avg): 3.4896378189325334\n",
            "Epoch: 2/3, trained_data:  49280 / 120000, Loss(avg): 3.217355954647064\n",
            "Epoch: 2/3, trained_data:  49600 / 120000, Loss(avg): 3.144516906142235\n",
            "Epoch: 2/3, trained_data:  49920 / 120000, Loss(avg): 2.8185366943478583\n",
            "Epoch: 2/3, trained_data:  50240 / 120000, Loss(avg): 2.8077512472867965\n",
            "Epoch: 2/3, trained_data:  50560 / 120000, Loss(avg): 3.8566267400979997\n",
            "Epoch: 2/3, trained_data:  50880 / 120000, Loss(avg): 2.8535909801721573\n",
            "Epoch: 2/3, trained_data:  51200 / 120000, Loss(avg): 3.054164482653141\n",
            "Epoch: 2/3, trained_data:  51520 / 120000, Loss(avg): 2.390338954329491\n",
            "Epoch: 2/3, trained_data:  51840 / 120000, Loss(avg): 2.6427481204271315\n",
            "Epoch: 2/3, trained_data:  52160 / 120000, Loss(avg): 2.8526044994592668\n",
            "Epoch: 2/3, trained_data:  52480 / 120000, Loss(avg): 2.890210847556591\n",
            "Epoch: 2/3, trained_data:  52800 / 120000, Loss(avg): 2.3696225464344023\n",
            "Epoch: 2/3, trained_data:  53120 / 120000, Loss(avg): 2.3643146730959415\n",
            "Epoch: 2/3, trained_data:  53440 / 120000, Loss(avg): 2.744154096767306\n",
            "Epoch: 2/3, trained_data:  53760 / 120000, Loss(avg): 3.0044044449925424\n",
            "Epoch: 2/3, trained_data:  54080 / 120000, Loss(avg): 3.2498999297618867\n",
            "Epoch: 2/3, trained_data:  54400 / 120000, Loss(avg): 2.4251118034124373\n",
            "Epoch: 2/3, trained_data:  54720 / 120000, Loss(avg): 2.871659849584103\n",
            "Epoch: 2/3, trained_data:  55040 / 120000, Loss(avg): 1.9848634880036116\n",
            "Epoch: 2/3, trained_data:  55360 / 120000, Loss(avg): 2.5708821758627893\n",
            "Epoch: 2/3, trained_data:  55680 / 120000, Loss(avg): 2.1900549195706844\n",
            "Epoch: 2/3, trained_data:  56000 / 120000, Loss(avg): 2.7034209474921225\n",
            "Epoch: 2/3, trained_data:  56320 / 120000, Loss(avg): 3.624563638865948\n",
            "Epoch: 2/3, trained_data:  56640 / 120000, Loss(avg): 3.605250844359398\n",
            "Epoch: 2/3, trained_data:  56960 / 120000, Loss(avg): 3.5150709062814713\n",
            "Epoch: 2/3, trained_data:  57280 / 120000, Loss(avg): 2.3047779843211176\n",
            "Epoch: 2/3, trained_data:  57600 / 120000, Loss(avg): 3.4643091797828673\n",
            "Epoch: 2/3, trained_data:  57920 / 120000, Loss(avg): 3.039143404364586\n",
            "Epoch: 2/3, trained_data:  58240 / 120000, Loss(avg): 2.3416893541812898\n",
            "Epoch: 2/3, trained_data:  58560 / 120000, Loss(avg): 3.551747405529022\n",
            "Epoch: 2/3, trained_data:  58880 / 120000, Loss(avg): 3.200507342815399\n",
            "Epoch: 2/3, trained_data:  59200 / 120000, Loss(avg): 2.7545337811112405\n",
            "Epoch: 2/3, trained_data:  59520 / 120000, Loss(avg): 2.826431173086166\n",
            "Epoch: 2/3, trained_data:  59840 / 120000, Loss(avg): 1.9350615300238132\n",
            "Epoch: 2/3, trained_data:  60160 / 120000, Loss(avg): 3.0168085604906083\n",
            "Epoch: 2/3, trained_data:  60480 / 120000, Loss(avg): 3.3216331124305727\n",
            "Epoch: 2/3, trained_data:  60800 / 120000, Loss(avg): 2.345726279914379\n",
            "Epoch: 2/3, trained_data:  61120 / 120000, Loss(avg): 2.2843282386660575\n",
            "Epoch: 2/3, trained_data:  61440 / 120000, Loss(avg): 2.357088588178158\n",
            "Epoch: 2/3, trained_data:  61760 / 120000, Loss(avg): 1.785841380804777\n",
            "Epoch: 2/3, trained_data:  62080 / 120000, Loss(avg): 2.045970305800438\n",
            "Epoch: 2/3, trained_data:  62400 / 120000, Loss(avg): 2.2103036634624003\n",
            "Epoch: 2/3, trained_data:  62720 / 120000, Loss(avg): 2.873201608657837\n",
            "Epoch: 2/3, trained_data:  63040 / 120000, Loss(avg): 2.567197676002979\n",
            "Epoch: 2/3, trained_data:  63360 / 120000, Loss(avg): 1.7003143787384034\n",
            "Epoch: 2/3, trained_data:  63680 / 120000, Loss(avg): 3.086624695919454\n",
            "Epoch: 2/3, trained_data:  64000 / 120000, Loss(avg): 3.354353928565979\n",
            "Epoch: 2/3, trained_data:  64320 / 120000, Loss(avg): 3.1876235604286194\n",
            "Epoch: 2/3, trained_data:  64640 / 120000, Loss(avg): 1.8260143980383874\n",
            "Epoch: 2/3, trained_data:  64960 / 120000, Loss(avg): 3.0868417721241714\n",
            "Epoch: 2/3, trained_data:  65280 / 120000, Loss(avg): 2.3830120101571084\n",
            "Epoch: 2/3, trained_data:  65600 / 120000, Loss(avg): 2.952485758066177\n",
            "Epoch: 2/3, trained_data:  65920 / 120000, Loss(avg): 3.5654757916927338\n",
            "Epoch: 2/3, trained_data:  66240 / 120000, Loss(avg): 2.435417352616787\n",
            "Epoch: 2/3, trained_data:  66560 / 120000, Loss(avg): 2.4016911342740057\n",
            "Epoch: 2/3, trained_data:  66880 / 120000, Loss(avg): 3.2680273562669755\n",
            "Epoch: 2/3, trained_data:  67200 / 120000, Loss(avg): 2.4549940094351768\n",
            "Epoch: 2/3, trained_data:  67520 / 120000, Loss(avg): 2.4342397093772887\n",
            "Epoch: 2/3, trained_data:  67840 / 120000, Loss(avg): 2.6682731792330743\n",
            "Epoch: 2/3, trained_data:  68160 / 120000, Loss(avg): 2.189632222056389\n",
            "Epoch: 2/3, trained_data:  68480 / 120000, Loss(avg): 3.152808617055416\n",
            "Epoch: 2/3, trained_data:  68800 / 120000, Loss(avg): 2.852787619829178\n",
            "Epoch: 2/3, trained_data:  69120 / 120000, Loss(avg): 3.4125994056463242\n",
            "Epoch: 2/3, trained_data:  69440 / 120000, Loss(avg): 2.3853917926549912\n",
            "Epoch: 2/3, trained_data:  69760 / 120000, Loss(avg): 2.3368200197815896\n",
            "Epoch: 2/3, trained_data:  70080 / 120000, Loss(avg): 3.490563078224659\n",
            "Epoch: 2/3, trained_data:  70400 / 120000, Loss(avg): 2.955345797538757\n",
            "Epoch: 2/3, trained_data:  70720 / 120000, Loss(avg): 3.9079088270664215\n",
            "Epoch: 2/3, trained_data:  71040 / 120000, Loss(avg): 3.447579303383827\n",
            "Epoch: 2/3, trained_data:  71360 / 120000, Loss(avg): 3.174207529425621\n",
            "Epoch: 2/3, trained_data:  71680 / 120000, Loss(avg): 2.613896298408508\n",
            "Epoch: 2/3, trained_data:  72000 / 120000, Loss(avg): 2.613710658252239\n",
            "Epoch: 2/3, trained_data:  72320 / 120000, Loss(avg): 2.107285995781422\n",
            "Epoch: 2/3, trained_data:  72640 / 120000, Loss(avg): 3.16373822465539\n",
            "Epoch: 2/3, trained_data:  72960 / 120000, Loss(avg): 2.806559120118618\n",
            "Epoch: 2/3, trained_data:  73280 / 120000, Loss(avg): 3.15658400952816\n",
            "Epoch: 2/3, trained_data:  73600 / 120000, Loss(avg): 2.7217559427022935\n",
            "Epoch: 2/3, trained_data:  73920 / 120000, Loss(avg): 2.3452450409531593\n",
            "Epoch: 2/3, trained_data:  74240 / 120000, Loss(avg): 3.8441728055477142\n",
            "Epoch: 2/3, trained_data:  74560 / 120000, Loss(avg): 2.3700803101062773\n",
            "Epoch: 2/3, trained_data:  74880 / 120000, Loss(avg): 2.3976751551032067\n",
            "Epoch: 2/3, trained_data:  75200 / 120000, Loss(avg): 3.3712227169424294\n",
            "Epoch: 2/3, trained_data:  75520 / 120000, Loss(avg): 2.7865569204092027\n",
            "Epoch: 2/3, trained_data:  75840 / 120000, Loss(avg): 2.299195337295532\n",
            "Epoch: 2/3, trained_data:  76160 / 120000, Loss(avg): 2.2425442025065423\n",
            "Epoch: 2/3, trained_data:  76480 / 120000, Loss(avg): 3.0206711694598196\n",
            "Epoch: 2/3, trained_data:  76800 / 120000, Loss(avg): 2.196412792801857\n",
            "Epoch: 2/3, trained_data:  77120 / 120000, Loss(avg): 2.703218813240528\n",
            "Epoch: 2/3, trained_data:  77440 / 120000, Loss(avg): 3.848559094965458\n",
            "Epoch: 2/3, trained_data:  77760 / 120000, Loss(avg): 2.4716145008802415\n",
            "Epoch: 2/3, trained_data:  78080 / 120000, Loss(avg): 2.0312036350369453\n",
            "Epoch: 2/3, trained_data:  78400 / 120000, Loss(avg): 3.3367501229047773\n",
            "Epoch: 2/3, trained_data:  78720 / 120000, Loss(avg): 3.0936697140336036\n",
            "Epoch: 2/3, trained_data:  79040 / 120000, Loss(avg): 2.8382190823554994\n",
            "Epoch: 2/3, trained_data:  79360 / 120000, Loss(avg): 2.576039966568351\n",
            "Epoch: 2/3, trained_data:  79680 / 120000, Loss(avg): 2.983620011806488\n",
            "Epoch: 2/3, trained_data:  80000 / 120000, Loss(avg): 2.0154743142426015\n",
            "Validation Accuracy: 0.8896\n",
            "Epoch: 3/3, trained_data:  80320 / 120000, Loss(avg): 1.1336814135313034\n",
            "Epoch: 3/3, trained_data:  80640 / 120000, Loss(avg): 0.7472230356186629\n",
            "Epoch: 3/3, trained_data:  80960 / 120000, Loss(avg): 1.2343451477587224\n",
            "Epoch: 3/3, trained_data:  81280 / 120000, Loss(avg): 1.4101098328828812\n",
            "Epoch: 3/3, trained_data:  81600 / 120000, Loss(avg): 0.8120796352624893\n",
            "Epoch: 3/3, trained_data:  81920 / 120000, Loss(avg): 1.911249717324972\n",
            "Epoch: 3/3, trained_data:  82240 / 120000, Loss(avg): 0.9497462090104818\n",
            "Epoch: 3/3, trained_data:  82560 / 120000, Loss(avg): 1.73674186617136\n",
            "Epoch: 3/3, trained_data:  82880 / 120000, Loss(avg): 1.420904441177845\n",
            "Epoch: 3/3, trained_data:  83200 / 120000, Loss(avg): 0.9698740173131227\n",
            "Epoch: 3/3, trained_data:  83520 / 120000, Loss(avg): 1.1830940403044223\n",
            "Epoch: 3/3, trained_data:  83840 / 120000, Loss(avg): 1.7060865534469485\n",
            "Epoch: 3/3, trained_data:  84160 / 120000, Loss(avg): 1.2036690719425678\n",
            "Epoch: 3/3, trained_data:  84480 / 120000, Loss(avg): 1.6484453041106462\n",
            "Epoch: 3/3, trained_data:  84800 / 120000, Loss(avg): 0.7273443365469575\n",
            "Epoch: 3/3, trained_data:  85120 / 120000, Loss(avg): 1.683235464990139\n",
            "Epoch: 3/3, trained_data:  85440 / 120000, Loss(avg): 1.4927002977579833\n",
            "Epoch: 3/3, trained_data:  85760 / 120000, Loss(avg): 1.4437942702323199\n",
            "Epoch: 3/3, trained_data:  86080 / 120000, Loss(avg): 1.705423653870821\n",
            "Epoch: 3/3, trained_data:  86400 / 120000, Loss(avg): 1.0617165111005307\n",
            "Epoch: 3/3, trained_data:  86720 / 120000, Loss(avg): 2.033637525513768\n",
            "Epoch: 3/3, trained_data:  87040 / 120000, Loss(avg): 1.1203621447086334\n",
            "Epoch: 3/3, trained_data:  87360 / 120000, Loss(avg): 1.21133034825325\n",
            "Epoch: 3/3, trained_data:  87680 / 120000, Loss(avg): 1.1133877828717231\n",
            "Epoch: 3/3, trained_data:  88000 / 120000, Loss(avg): 0.8914280453696847\n",
            "Epoch: 3/3, trained_data:  88320 / 120000, Loss(avg): 1.2271692715585232\n",
            "Epoch: 3/3, trained_data:  88640 / 120000, Loss(avg): 1.4912158470600843\n",
            "Epoch: 3/3, trained_data:  88960 / 120000, Loss(avg): 1.3469404930248856\n",
            "Epoch: 3/3, trained_data:  89280 / 120000, Loss(avg): 1.4031844444572925\n",
            "Epoch: 3/3, trained_data:  89600 / 120000, Loss(avg): 1.0225751351565122\n",
            "Epoch: 3/3, trained_data:  89920 / 120000, Loss(avg): 1.2306184589862823\n",
            "Epoch: 3/3, trained_data:  90240 / 120000, Loss(avg): 0.8441100498661399\n",
            "Epoch: 3/3, trained_data:  90560 / 120000, Loss(avg): 1.551479552127421\n",
            "Epoch: 3/3, trained_data:  90880 / 120000, Loss(avg): 1.5300100089982152\n",
            "Epoch: 3/3, trained_data:  91200 / 120000, Loss(avg): 2.297794206440449\n",
            "Epoch: 3/3, trained_data:  91520 / 120000, Loss(avg): 1.263839554041624\n",
            "Epoch: 3/3, trained_data:  91840 / 120000, Loss(avg): 1.3231708899140358\n",
            "Epoch: 3/3, trained_data:  92160 / 120000, Loss(avg): 0.9710566248744726\n",
            "Epoch: 3/3, trained_data:  92480 / 120000, Loss(avg): 1.8513096293434501\n",
            "Epoch: 3/3, trained_data:  92800 / 120000, Loss(avg): 1.4516150787472726\n",
            "Epoch: 3/3, trained_data:  93120 / 120000, Loss(avg): 0.9600740399211645\n",
            "Epoch: 3/3, trained_data:  93440 / 120000, Loss(avg): 1.6584124136716127\n",
            "Epoch: 3/3, trained_data:  93760 / 120000, Loss(avg): 1.2012691546231509\n",
            "Epoch: 3/3, trained_data:  94080 / 120000, Loss(avg): 1.924176700040698\n",
            "Epoch: 3/3, trained_data:  94400 / 120000, Loss(avg): 1.946378043293953\n",
            "Epoch: 3/3, trained_data:  94720 / 120000, Loss(avg): 1.2699423007667066\n",
            "Epoch: 3/3, trained_data:  95040 / 120000, Loss(avg): 1.8516043044626713\n",
            "Epoch: 3/3, trained_data:  95360 / 120000, Loss(avg): 1.5887446761131288\n",
            "Epoch: 3/3, trained_data:  95680 / 120000, Loss(avg): 1.3806243062019348\n",
            "Epoch: 3/3, trained_data:  96000 / 120000, Loss(avg): 0.9589751917868853\n",
            "Epoch: 3/3, trained_data:  96320 / 120000, Loss(avg): 1.3169707011431455\n",
            "Epoch: 3/3, trained_data:  96640 / 120000, Loss(avg): 1.5911343837156893\n",
            "Epoch: 3/3, trained_data:  96960 / 120000, Loss(avg): 1.097421359270811\n",
            "Epoch: 3/3, trained_data:  97280 / 120000, Loss(avg): 1.2051613043993712\n",
            "Epoch: 3/3, trained_data:  97600 / 120000, Loss(avg): 1.1987564265727997\n",
            "Epoch: 3/3, trained_data:  97920 / 120000, Loss(avg): 1.1308565229177474\n",
            "Epoch: 3/3, trained_data:  98240 / 120000, Loss(avg): 0.9301166579127311\n",
            "Epoch: 3/3, trained_data:  98560 / 120000, Loss(avg): 2.0745551070198416\n",
            "Epoch: 3/3, trained_data:  98880 / 120000, Loss(avg): 1.5746770199388265\n",
            "Epoch: 3/3, trained_data:  99200 / 120000, Loss(avg): 1.2263957012444735\n",
            "Epoch: 3/3, trained_data:  99520 / 120000, Loss(avg): 1.5009278137236834\n",
            "Epoch: 3/3, trained_data:  99840 / 120000, Loss(avg): 1.4731124419718982\n",
            "Epoch: 3/3, trained_data:  100160 / 120000, Loss(avg): 1.4012073814868926\n",
            "Epoch: 3/3, trained_data:  100480 / 120000, Loss(avg): 0.7535978183150291\n",
            "Epoch: 3/3, trained_data:  100800 / 120000, Loss(avg): 1.1191619275137783\n",
            "Epoch: 3/3, trained_data:  101120 / 120000, Loss(avg): 1.7523710906505585\n",
            "Epoch: 3/3, trained_data:  101440 / 120000, Loss(avg): 0.9381784349679947\n",
            "Epoch: 3/3, trained_data:  101760 / 120000, Loss(avg): 0.6594863664358854\n",
            "Epoch: 3/3, trained_data:  102080 / 120000, Loss(avg): 0.7687441755086184\n",
            "Epoch: 3/3, trained_data:  102400 / 120000, Loss(avg): 0.8424956081435084\n",
            "Epoch: 3/3, trained_data:  102720 / 120000, Loss(avg): 1.3003321481868624\n",
            "Epoch: 3/3, trained_data:  103040 / 120000, Loss(avg): 0.7954986972734333\n",
            "Epoch: 3/3, trained_data:  103360 / 120000, Loss(avg): 1.3887697735801339\n",
            "Epoch: 3/3, trained_data:  103680 / 120000, Loss(avg): 1.0872728087008\n",
            "Epoch: 3/3, trained_data:  104000 / 120000, Loss(avg): 1.2151730431243777\n",
            "Epoch: 3/3, trained_data:  104320 / 120000, Loss(avg): 1.2268326997756958\n",
            "Epoch: 3/3, trained_data:  104640 / 120000, Loss(avg): 1.5982544647529722\n",
            "Epoch: 3/3, trained_data:  104960 / 120000, Loss(avg): 0.7029898962005973\n",
            "Epoch: 3/3, trained_data:  105280 / 120000, Loss(avg): 1.2693244185298682\n",
            "Epoch: 3/3, trained_data:  105600 / 120000, Loss(avg): 1.1645912213250995\n",
            "Epoch: 3/3, trained_data:  105920 / 120000, Loss(avg): 1.2387508679181338\n",
            "Epoch: 3/3, trained_data:  106240 / 120000, Loss(avg): 0.9070505738258362\n",
            "Epoch: 3/3, trained_data:  106560 / 120000, Loss(avg): 1.107169068045914\n",
            "Epoch: 3/3, trained_data:  106880 / 120000, Loss(avg): 0.9283644600771368\n",
            "Epoch: 3/3, trained_data:  107200 / 120000, Loss(avg): 2.204290560632944\n",
            "Epoch: 3/3, trained_data:  107520 / 120000, Loss(avg): 1.4730492550879717\n",
            "Epoch: 3/3, trained_data:  107840 / 120000, Loss(avg): 1.5559884887188673\n",
            "Epoch: 3/3, trained_data:  108160 / 120000, Loss(avg): 1.2987352225929498\n",
            "Epoch: 3/3, trained_data:  108480 / 120000, Loss(avg): 1.1349047891795636\n",
            "Epoch: 3/3, trained_data:  108800 / 120000, Loss(avg): 1.720746885985136\n",
            "Epoch: 3/3, trained_data:  109120 / 120000, Loss(avg): 0.5920968573540449\n",
            "Epoch: 3/3, trained_data:  109440 / 120000, Loss(avg): 2.163465627282858\n",
            "Epoch: 3/3, trained_data:  109760 / 120000, Loss(avg): 1.0875673746690153\n",
            "Epoch: 3/3, trained_data:  110080 / 120000, Loss(avg): 1.242923913896084\n",
            "Epoch: 3/3, trained_data:  110400 / 120000, Loss(avg): 1.7971831738948822\n",
            "Epoch: 3/3, trained_data:  110720 / 120000, Loss(avg): 1.135998760163784\n",
            "Epoch: 3/3, trained_data:  111040 / 120000, Loss(avg): 1.006351063773036\n",
            "Epoch: 3/3, trained_data:  111360 / 120000, Loss(avg): 1.7903382927179337\n",
            "Epoch: 3/3, trained_data:  111680 / 120000, Loss(avg): 1.6570849768817424\n",
            "Epoch: 3/3, trained_data:  112000 / 120000, Loss(avg): 1.3061573758721352\n",
            "Epoch: 3/3, trained_data:  112320 / 120000, Loss(avg): 1.060906568914652\n",
            "Epoch: 3/3, trained_data:  112640 / 120000, Loss(avg): 1.3362947701476515\n",
            "Epoch: 3/3, trained_data:  112960 / 120000, Loss(avg): 1.4768613025546073\n",
            "Epoch: 3/3, trained_data:  113280 / 120000, Loss(avg): 1.4992601983249187\n",
            "Epoch: 3/3, trained_data:  113600 / 120000, Loss(avg): 1.3002531386911869\n",
            "Epoch: 3/3, trained_data:  113920 / 120000, Loss(avg): 1.3738859737291933\n",
            "Epoch: 3/3, trained_data:  114240 / 120000, Loss(avg): 2.1795391786843537\n",
            "Epoch: 3/3, trained_data:  114560 / 120000, Loss(avg): 1.1180705923587084\n",
            "Epoch: 3/3, trained_data:  114880 / 120000, Loss(avg): 0.8195525728166103\n",
            "Epoch: 3/3, trained_data:  115200 / 120000, Loss(avg): 1.2678881525993346\n",
            "Epoch: 3/3, trained_data:  115520 / 120000, Loss(avg): 1.2239516034722329\n",
            "Epoch: 3/3, trained_data:  115840 / 120000, Loss(avg): 1.2224433396011591\n",
            "Epoch: 3/3, trained_data:  116160 / 120000, Loss(avg): 2.259323112294078\n",
            "Epoch: 3/3, trained_data:  116480 / 120000, Loss(avg): 1.2745436042547227\n",
            "Epoch: 3/3, trained_data:  116800 / 120000, Loss(avg): 1.0746434394270181\n",
            "Epoch: 3/3, trained_data:  117120 / 120000, Loss(avg): 1.4826661679893731\n",
            "Epoch: 3/3, trained_data:  117440 / 120000, Loss(avg): 1.4228821269236505\n",
            "Epoch: 3/3, trained_data:  117760 / 120000, Loss(avg): 1.7921374209225178\n",
            "Epoch: 3/3, trained_data:  118080 / 120000, Loss(avg): 1.3313290782272815\n",
            "Epoch: 3/3, trained_data:  118400 / 120000, Loss(avg): 0.9948654491454363\n",
            "Epoch: 3/3, trained_data:  118720 / 120000, Loss(avg): 1.031103201583028\n",
            "Epoch: 3/3, trained_data:  119040 / 120000, Loss(avg): 1.8463037859648466\n",
            "Epoch: 3/3, trained_data:  119360 / 120000, Loss(avg): 1.3357374608516692\n",
            "Epoch: 3/3, trained_data:  119680 / 120000, Loss(avg): 1.2445174649357795\n",
            "Epoch: 3/3, trained_data:  120000 / 120000, Loss(avg): 1.163378495723009\n",
            "Validation Accuracy: 0.8860\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/sentiment_analysis_dataset/model/bert_model.pth')"
      ],
      "metadata": {
        "id": "6kRNx0l5nOZc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "# 创建一个新的模型实例\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# 加载保存的权重\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/sentiment_analysis_dataset/model/bert_model.pth'))"
      ],
      "metadata": {
        "id": "MDjnZyU38pgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = MyDataset(X_test, y_test, tokenizer, max_len)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1)\n",
        "evaluate_model(model, test_loader, device)"
      ],
      "metadata": {
        "id": "qMO4swl58yWi",
        "outputId": "4fba4b25-2cbc-45cf-ba39-b8cfbafd5e29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.895"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6vgKvNIn9UDj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "欢迎使用 Colaboratory",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}